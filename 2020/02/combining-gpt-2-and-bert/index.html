<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Combining GPT-2 and BERT to make a fake person | bonkerfield</title><meta name="description" content="just your standard information identity spacetime field"><meta itemprop="name" content="will stedden"><meta itemprop="description" content="just your standard information identity spacetime field"><meta itemprop="image" content="https://bonkerfield.org/assets/images/2020/thiscommentisgreat.png"><meta property="og:url" content="https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/"><meta property="og:type" content="website"><meta property="og:title" content="Combining GPT-2 and BERT to make a fake person | bonkerfield"><meta property="og:site_name" content="bonkerfield"><meta property="og:description" content="just your standard information identity spacetime field"><meta property="og:image" content="https://bonkerfield.org/assets/images/2020/thiscommentisgreat.png"><meta property="twitter:url" content="https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/"><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Combining GPT-2 and BERT to make a fake person | bonkerfield"><meta property="twitter:site" content="bonkerfield"><meta property="twitter:description" content="just your standard information identity spacetime field"><meta property="twitter:creator" content="@bonkerfield"><meta property="twitter:image" content="https://bonkerfield.org/assets/images/2020/thiscommentisgreat.png"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="bonkerfield" href="/feed.xml"><link rel="canonical" href="/2020/02/combining-gpt-2-and-bert/"><link href="https://fonts.googleapis.com/css?family=Exo" rel="stylesheet"><link rel="webmention" href="https://webmention.io/bonkerfield.org/webmention" /><link rel="pingback" href="https://webmention.io/bonkerfield.org/xmlrpc" /> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-132451521-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-132451521-2'); </script><link href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css' rel='stylesheet'/></head><body id="combining-gpt-2-and-bert-to-make-a-fake-person" class="post-layout"><header class="header"> <span><sup><a class="header__leadin" href="https://will.stedden.org">will.stedden ></a> </sup> <a class="header__title" href="/">bonkerfield</a></span><nav><ul class="header__list"><li><a href="/things">things</a></li><li><a href="/doings">doings</a></li><li><a href="/reasons">reasons</a></li><li><a href="https://viewfoil.bonkerfield.org/"><i class="fa fa-envelope"></i></a></li><li><a href="/search"><i class="fa fa-search"></i></a></li></ul></nav></header><a style="display:none;" class="p-author h-card" href="http://bonkerfield.org"> <img alt="" src="/assets/images/main/face_prof.jpg"/> will stedden</a><main class="bonk"><div class="post"><article class="h-entry" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding-half"><div class="grid-small"><h2 itemprop="name headline p-name">Combining GPT-2 and BERT to make a fake person</h2><time class="post__date dt-published" datetime="2020-02-04T00:00:00-07:00" itemprop="datePublished">2020·02·04</time> <span class="post__author"> | <a class="p-name u-url" href="http://will.stedden.org" title="about">will stedden</a> </span><p class="post__description p-summary"></p><a class="u-url" style="display:none;" href="https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/"></a></div></div><div class="post__content section-padding"><div class="grid"><div class="e-content" id="markdown" itemprop="articleBody"><p> <a href="https://openai.com/blog/better-language-models/">GPT-2</a> is a deep learning model that is able to generate astonishingly coherent English text. It was released last year, and <a href="https://towardsdatascience.com/openais-gpt-2-the-model-the-hype-and-the-controversy-1109f4bfd5e8">everyone’s</a> <a href="https://www.fast.ai/2019/02/15/openai-gp2/">mind</a> was <a href="https://www.theguardian.com/commentisfree/2019/feb/15/ai-write-robot-openai-gpt2-elon-musk">blown</a> into <a href="http://approximatelycorrect.com/2019/02/17/openai-trains-language-model-mass-hysteria-ensues/">histrionic</a> <a href="https://www.wired.com/story/ai-text-generator-too-dangerous-to-make-public/">hyperbole</a>, including mine. Its creators at <a href="https://openai.com/">OpenAI</a> were so impressed by the model's performance that they originally didn't release it for fear of it being too easy to abuse. I think they were right to be concerned. Here is an excerpt that the model generated, taken from their <a href="https://openai.com/blog/better-language-models/#sample1">release page</a>.</p><blockquote> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.</blockquote><p><a href="https://openai.com/blog/better-language-models/#sample1">read more</a></p><p> When I saw what GPT-2 was capable of generating, I had chills. We are now very close to effectively simulating human creativity. I find machine imitation of human communication fascinating; in fact, it's something I've explored in <a href="https://a.ttent.io/n/">my fiction writing</a> previously. But since I've never worked on natural language generation or deep learning, I decided to look more closely at just what this machine could do.</p><h4>The person you are speaking with is not real</h4><p> My goal was to see how close I could come to impersonating a real human with algorithmically generated text and almost no manual quality control.</p><p> I decided that one of the easiest places to test such a system would be in the responses to comments on the social media website, <a href="https://www.reddit.com/">reddit</a>. My goal became to generate a bot that would respond topically to comments, garner upvotes, and see if it can promote discussion. In case you are worried about the ethicality of releasing a surreptitious human on reddit, rest assured I have only deployed the bot sparingly to avoid generating too much annoyance in the world. And I have manually reviewed evey comment to ensure that it produced nothing too offensive.</p><p> Honestly, I was hoping I could use this tool to become a little more popular on this whole internet thing. I've been pretty much terrible at interacting on social media so I figured maybe I could automate the problem away. I quickly learned that just using GPT-2 on it's own is not quite adequate to impersonate a human most of the time. But with a little modification, I've found that building a frighteningly passable reddit commenter is not only possible; it's pretty easy.</p><h4 id="gpt2shortcoming">The Shortcoming of GPT-2</h4><p> What GPT-2's creators fail to mention is that while almost everything the model generates is grammatically and syntactically correct, only a tiny fraction of the outputs make any damn sense. Here is another excerpt that shows just how non-human the output normally looks.</p><blockquote class="twitter-tweet" data-lang="en" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">Here&#39;s a short story i generated using OpenAI&#39;s GPT-2 tool (prompt in bold) <a href="https://t.co/DGIVwGuAUV">pic.twitter.com/DGIVwGuAUV</a></p>&mdash; will knight (@willknight) <a href="https://twitter.com/willknight/status/1096134045774344199?ref_src=twsrc%5Etfw">February 14, 2019</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p> When I first started experimenting, I generated a lot of similar gibberish. As it turns out, GPT2 on its own is fairly prone to getting into weird unintelligible rants. Here are some examples.</p><blockquote> The idea of the film is very similar to the kind of film we’ve seen before, “The Road”. The film “took place” at “a time”, “at a place”. “The Road” was set at “a time”, “a place”.” “The Road” was also set at a time of the Soviet Union’s collapse,””“a time”, “the collapse” of communism”. “The Road” is set at a time of the Russian Revolution,”“the collapse” of the Soviet Union’s fall,”“a time”, “the collapse” of the Soviet Union’s collapse.” So, “The Road” is a film about the dissolution of the Soviet Union’s grip on the world,”“a time of the Russian Revolution”.</blockquote><p>Clearly the algorithm is getting confused on the way quotations work. Then there's this one, which makes grammatical sense, but is clearly a series of statements that no regular person would ever say (unless they were trolling).</p><blockquote> I still can't believe you. I had the same birthday last year. I'm a 10 year old, and I have never even heard of this novel.</blockquote><p> Worse still, a lot of the time, GPT2 will just start repeating a few crazy phrases over and over. You can <a href="https://docs.google.com/spreadsheets/d/1n2_IM32ULuu_x9f_hX2YlmRfahF61K_os5fGg2RcgBA/edit?usp=sharing">check out some of the model's output</a> to get a taste of the kinds of things that it generates in the raw.</p><p> I wouldn't want to build a bot that spewed crazy looking responses like that all the time. It would be incredibly annoying to other redditors and would probably be flagged right away. Still, I didn't want to give up on the idea completely. I started brainstorming about ways that I could fix the performance problems with GPT2 and make it more robust, and I came up with something that was able to filter out a lot of the crap responses.</p><h5>Machines trying to trick other machines</h5><p> To fix the problem, I borrowed an idea from another deep learning architecture called a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial network</a> or GAN. GANs have been used extensively in the past and have been astonishingly successful in impersonating <a href="https://www.thispersondoesnotexist.com/">images</a>, <a href="https://magenta.tensorflow.org/gansynth">music</a>, and even <a href="https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10?gi=e90e56af6387">text</a> (though it doesn't do text that well). The rise of the <a href="https://en.wikipedia.org/wiki/Deepfake#History">"deep fake"</a> is mostly thanks to developments in the GAN architecture.</p><p> The concept of the GAN is pretty simple. You train two algorithms, one to generate text (generator), and another to try to distinguish the generator’s text from human text (discriminator). These algorithms are typically called <a href=”https://en.wikipedia.org/wiki/Language_model”>language models</a> because they attempt to model the way language is produced. In a classical GAN you then use the two models to improve each other by having the generator constantly compete to trick the discriminator (hence Adversarial).</p><img title="GAN illustration" src="/assets/images/2020/gan_explain.png" alt="GAN illustration"/><p style="text-align:center;"><em>GAN diagram (<a href="https://developers.google.com/machine-learning/gan/gan_structure">source</a>)</em></p><p> It's a very intuitive and clever concept, and one that I personally feel mirrors the <a href="https://en.wikipedia.org/wiki/Bicameralism_(psychology)">internal dialog</a> that I constantly have in my own brain's decision making system. The critic in my head feels almost like a discriminator algorithm <a href="https://i.imgur.com/mGva0nK.gif">throwing shade</a> on my internal generator algorithm. Anyway, if you're interested in how they work in detail, you can read more <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">here</a>.</p><p> Unfortunately, I wasn't quite smart enough to figure out how to modify the pre-existing GPT-2 model to turn it into a GAN. I think it's possible, but tensorflow is a <a href="https://nostalgebraist.tumblr.com/post/189464877164/attention-conservation-notice-machine-learning">confusing beast</a>, and I'm not yet at the point where I care enough to untangle that mess. Instead, I did something a little simpler that was just effective enough to make the results passable.</p><h4>Filtered generator -> discriminator method</h4><p> I instead opted for a multi-stage modeling framework, utilizing three separate deep-learning models stitched together one after the next. This diagram illustrates the many parts that needed to be trained, and how they were strung together to produce replies from comments.</p><img title="GPT2 BERT commentor workflow" src="/assets/images/2020/gpt2_bert_workflow.png" alt="GPT2 BERT comment system workflow"/><p> In this setup, I first pick a comment on reddit to serve as seed text for the generator. I generate a whole bunch of replies for this comment using my GPT-2 model. Then I pass all the candidates to the discriminator model to filter out the messed up comments and only select the best ones.</p><p> To build the discriminators, I fine-tuned another deep-learning language model called <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT</a>. I made two models, one for how realistic the reply was and another for how many upvotes the reply would get.</p><p> You can read <a href="/2020/02/reddit-bot-gpt2-bert/">this post</a> for a detailed walkthrough of how the whole system was constructed, trained, tested, and deployed.</p><p> Just by looking at the initial results, it seemed likely that the bot was going to be able to communicate pretty convincingly. But the only <a href="https://en.wikipedia.org/wiki/Turing_test">real test</a> was to put it into use and see how people respond.</p><h4>Meet tupperware-party</h4><p> Once I had the models built and hooked together, the last step was to plug the bot into reddit. I made an account for the bot called <strong>tupperware-party</strong>, which I figured sounded innocuous enough. I used <a href="https://praw.readthedocs.io/en/latest/">praw</a> to submit the replies automatically, and then I went through and examined all of them to make sure none were too offensive or annoying. As I was reading through the results, there were so many gems that it's hard to pick just a few examples to share.</p><h5 id='replies'>What did the robot say?</h5><p> This <a href="https://www.reddit.com/r/sciencefiction/comments/evqiti/dune_logo_unveiled_at_event_copyright_claimants/fg44yzw/?context=3">first one</a> seems like a perfect imitation of someone with a strong opinion on the internet.</p><img title="reddit-gpt2-bert-bot comment" src="/assets/images/2020/tupperware-party1.png" alt="COMMENT: 'Dune’s fandom is old and intense, and a rich thread in the cultural fabric of the internet generation' BOT_REPLY:'Dune’s fandom is overgrown, underfunded, and in many ways, a poor fit for the new, faster internet generation.'" style="border:2px;"/><p> I actually can't explain how <a href="https://www.reddit.com/r/BurningMan/comments/ep6pyq/playa_lung/feilsjn/?context=8&depth=9">this next one</a> could possibly work. It seems as if the bot is responding to the specific numerical bullet points in the original comment.</p><img title="reddit-gpt2-bert-bot comment" src="/assets/images/2020/tupperware-party2.png" alt="bot responds to specific numerical bullet point in source comment"/><p> Notice that in the original comment, point 2 is about sleep and the bot says "2" right before talking about sleep. Then it says "3" before switching subjects to talking about something that induces anxiety. It doesn't make perfect sense but it somehow knows to respond to bullet points separately, which seems like a huge leap given that it was never trained to do that specifically.</p><p> This <a href="https://www.reddit.com/r/artificial/comments/ep26lc/is_china_going_to_overtake_the_us_in_data_science/feik6wg/?context=8&depth=9"> one</a> is great on a number of levels. First, it's kind of meta because the bot was posting into the <a href="https://www.reddit.com/r/artificial/">r/artifical subreddit</a>, which is a forum dedicated to artificial intelligence. Second, not only is the comment pretty darn coherent, it is so much so that the original author writes a well thought out reply further expanding on his point in light of the bot's comment.</p><img title="reddit-gpt2-bert-bot comment" src="/assets/images/2020/tupperware-party3.png" alt="gpt2-bert on China"/><p> I honestly don't even know what to say to that. Is it possible that every conversation on the internet right now has at least one slightly ill-informed bot in the mix. We are seriously screwed. But don't worry, this bot is at least a little woke too already.</p><img title="reddit-gpt2-bert-bot comment" src="/assets/images/2020/tupperware-party4.png" alt="The first thing I think of when thinking about a villain's face turn is probably that they are a male character. Some males are actually pretty bad in the media..."/><p> There are so many surprisingly realistic replies that I enourage you to go through <a href="https://www.reddit.com/user/tupperware-party/comments/">tupperware-party's whole comment list</a>. Overall, the bot wrote 80 replies and 24 of them received at least one upvote. I'm impressed with that and hoping that maybe it will eventually be able to help <a href="https://www.reddit.com/user/bonkerfield">me</a> become more popular on reddit.</p><p> On the other hand, the single most popular comment (with 8 votes) was <a href="https://www.reddit.com/r/sciencefiction/comments/efej56/the_problem_with_the_original_dune_movie/fc16yq8/?context=3">this one</a>, which is just innocuous flattery.</p><img title="reddit-gpt2-bert-bot comment" src="/assets/images/2020/thiscommentisgreat.png" alt="This comment is great."/><p> Since this could easily be copy and pasted to every other comment and still be totally in context, I guess maybe I didn't need to try so hard.</p><h4 id="ethics">You can build one too!</h4><p>If you find this interesting, I've written a <a href="/2020/02/reddit-bot-gpt2-bert/">tutorial post</a> with details and code describing how I built everything and showing what you'd need to do to recreate one of your own. I realize there are definite ethicality concerns with building and using something like this so I encourage you to be an <a href="https://en.wikiquote.org/wiki/Bill_%26_Ted%27s_Excellent_Adventure">excellent</a> human and only use this tool sparingly and for that which <a href="https://en.wikipedia.org/wiki/Categorical_imperative">you deem to be good</a>.</p><h5>Ethical concerns</h5><p> I know there are definitely some ethical considerations when creating something like this. The reason I'm presenting it is because I actually think it is <a href="https://www.wired.com/story/company-wants-billions-make-ai-safe-humanity/">better</a> for more people to know about and be able to grapple with this kind of technology. If just a few people know about the capacity of these machines, then it is more likely that those small groups of people can abuse their advantage.</p><p> I also think that this technology is going to change the way we think about what's important about being human. After all, if a computer can effectively automate the paper-pushing jobs we've constructed and all the bullshit we create on the internet to distract us, then maybe it'll be time for us to move on to something more meaningful.</p><p> If you think what I've done is a problem feel free to <a href="https://viewfoil.bonkerfield.org">send me a message</a>, or publically shame me on <a href="https://twitter.com/bonkerfield">Twitter</a>.</p></div></div></div><div class="section-padding--none"><div class="grid"><hr class="sep" /></div></div><div class="section-padding"><div class="grid-small"><h3 class="post__discussion">Discussion Around the <a href="https://indieweb.org/Webmention">Web</a></h3><div id="webmentions"></div><hr style="margin: 5px 0;" class="sep" /><p>Join the Conversation</p><form target="_blank" name="tweet" action="https://twitter.com/share" method="get"> <input type="hidden" name="original_referer" value="https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/"> <input type="hidden" name="source" value="bonkerfield"> <textarea style="border-color:#aaa;" id="textar" name="text" maxlength="140"></textarea> <input type="hidden" name="url" value="https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/"> <button class="btn btn-sm btn-social btn-tw" type="submit" id="button"> <i class="fab fa-twitter"></i> <br/>comment by tweet </button> </form><hr style="margin: 5px 0;" class="sep" /> <form target="_blank" name="tweet" action="https://www.reddit.com/submit" method="get"> <input type="hidden" name="source" value="bonkerfield"> <textarea style="border-color:#aaa;" id="textar" name="title" maxlength="140"></textarea> <button class="btn btn-sm btn-social btn-rd" type="submit" id="button"> <i class="fab fa-reddit-alien"></i> <br/>submit to reddit </button> <input type="hidden" name="url" value="https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/"> </form> <script src="/assets/js/webmention.min.js"></script></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="https://bonkerfield.org/2021/01/story2hallucination/"><figure class="related__img"> <img src="/assets/images/2021/octo.gif" alt="Story2Hallucination: converting stories to deep learning GAN hallucinated animations" /></figure><div><h2 class="related__text">Story2Hallucination: converting stories to deep learning GAN hallucinated animations</h2></div></a></article><article class="related__post"> <a class="related__link" href="https://bonkerfield.org/2020/04/twenty-minute-gpt2-reply-bot/"><figure class="related__img"> <img src="/assets/images/2020/tup_too_real.png" alt="A GPT2+BERT reddit reply bot in 20 minutes" /></figure><div><h2 class="related__text">A GPT2+BERT reddit reply bot in 20 minutes</h2></div></a></article><article class="related__post"> <a class="related__link" href="https://bonkerfield.org/2020/02/reddit-bot-gpt2-bert/"><div><h2 class="related__text">How to build a convincing reddit personality with GPT2 and BERT</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get an update when I post new content.</p><form method="POST" action="https://bonkerfield.us4.list-manage.com/subscribe/post-json?u=68d1146ea8fe327def203f254&amp;id=e88ba623c5&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="b_68d1146ea8fe327def203f254_e88ba623c5" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__social"><li><a href="https://twitter.com/bonkerfield" rel="me" target="_blank"><i class="fab fa-twitter"></i></a></li><li><a href="https://www.linkedin.com/in/willstedden" rel="me" target="_blank"><i class="fab fa-linkedin"></i></a></li><li><a href="https://github.com/stedn" rel="me" target="_blank"><i class="fab fa-github"></i></a></li><li><a href="https://codepen.io/stedn/" rel="me" target="_blank"><i class="fab fa-codepen"></i></a></li><li><a href="https://www.reddit.com/user/bonkerfield" rel="me" target="_blank"><i class="fab fa-reddit-alien"></i></a></li><li><a href="https://fosstodon.org/@bonkerfield" rel="me" target="_blank"><i class="fab fa-mastodon"></i></a></li><li><a href="https://www.are.na/will-stedden" rel="me" target="_blank"><svg height='16px' id='svg4' inkscape:version='1.0.1 (1.0.1+r75)' sodipodi:docname='arena.svg' version='1.1' viewBox='0 0 150.38 88.986' xmlns='http://www.w3.org/2000/svg' xmlns:cc='http://creativecommons.org/ns#' xmlns:dc='http://purl.org/dc/elements/1.1/' xmlns:inkscape='http://www.inkscape.org/namespaces/inkscape' xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#' xmlns:sodipodi='http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd' xmlns:svg='http://www.w3.org/2000/svg'><metadata id='metadata10'> <rdf:RDF> <cc:Work rdf:about=''> <dc:format>image/svg+xml</dc:format> <dc:type rdf:resource='http://purl.org/dc/dcmitype/StillImage'/> <dc:title/> </cc:Work> </rdf:RDF> </metadata> <defs id='defs8'/> <sodipodi:namedview bordercolor='#666666' borderopacity='1' gridtolerance='10' guidetolerance='10' id='namedview6' inkscape:current-layer='svg4' inkscape:cx='52.509912' inkscape:cy='50.941261' inkscape:pageopacity='0' inkscape:pageshadow='2' inkscape:window-height='719' inkscape:window-maximized='1' inkscape:window-width='1366' inkscape:window-x='0' inkscape:window-y='25' inkscape:zoom='6.203223' objecttolerance='10' pagecolor='#ffffff' showgrid='false'/><path d='M148.93 62.356l-20.847-16.384c-1.276-1-1.276-2.642 0-3.645l20.848-16.38c1.28-1.002 1.815-2.695 1.19-3.76-.626-1.062-2.374-1.44-3.88-.84l-24.79 9.874c-1.507.606-2.927-.22-3.153-1.83L114.57 2.926C114.34 1.317 113.13 0 111.877 0c-1.247 0-2.456 1.317-2.68 2.925l-3.73 26.467c-.228 1.61-1.646 2.434-3.155 1.83l-24.38-9.71c-1.512-.602-3.975-.602-5.483 0l-24.384 9.71c-1.508.604-2.928-.22-3.154-1.83L41.186 2.925C40.956 1.317 39.748 0 38.5 0c-1.252 0-2.463 1.317-2.688 2.925l-3.73 26.467c-.226 1.61-1.645 2.434-3.153 1.83L4.14 21.35c-1.507-.603-3.252-.223-3.878.838-.625 1.066-.092 2.76 1.184 3.76l20.85 16.38c1.277 1.003 1.277 2.645 0 3.646L1.446 62.356C.166 63.358-.364 65.152.26 66.34c.627 1.19 2.372 1.668 3.877 1.064l24.567-9.866c1.51-.603 2.914.218 3.125 1.828l3.544 26.696c.214 1.607 1.618 2.923 3.12 2.923 1.5 0 2.905-1.315 3.12-2.923l3.55-26.696c.21-1.61 1.62-2.43 3.122-1.828l24.164 9.698c1.506.606 3.97.606 5.477 0l24.16-9.698c1.504-.603 2.91.218 3.125 1.828l3.55 26.696c.212 1.607 1.617 2.923 3.115 2.923 1.502 0 2.907-1.315 3.12-2.923l3.55-26.696c.216-1.61 1.62-2.43 3.124-1.828l24.57 9.866c1.5.604 3.25.125 3.876-1.063.627-1.186.094-2.98-1.185-3.982zM95.89 46.18L77.53 60.315c-1.285.99-3.393.99-4.674 0L54.49 46.18c-1.284-.99-1.294-2.62-.02-3.625l18.4-14.493c1.274-1.005 3.363-1.005 4.638 0l18.4 14.493c1.277 1.004 1.267 2.634-.02 3.626z' id='path2' style='fill:#ffffff'/> </svg></a></li></ul><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank"> <input type="hidden" name="cmd" value="_donations" /> <input type="hidden" name="business" value="3LHDHWRBWMGZA" /> <input type="hidden" name="item_name" value="Help Fund the Quantum Multiverse Bifurcator" /> <input type="hidden" name="currency_code" value="USD" /> <input type="submit" value="donate" name="submit" style="width:15%;min-width:70px;"> <img alt="" border="0" src="https://www.paypal.com/en_US/i/scr/pixel.gif" width="1" height="1" /> </form><div class="posts__pagination"> <span>All content on this blog is free to use under a creative commons <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC-BY-SA</a> license</span></div></div></div></footer><script src="/assets/js/app.min.js"></script> <script> (function() { var links = document.getElementsByTagName('a'); for (var i = 0; i < links.length; i++) { if (/^(http?:)?\/\//.test(links[i].getAttribute('href'))) { links[i].target = '_blank'; } } })(); </script></body></html>
  